{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9728247,"sourceType":"datasetVersion","datasetId":5950167}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-26T17:40:35.344122Z","iopub.execute_input":"2024-10-26T17:40:35.344820Z","iopub.status.idle":"2024-10-26T17:40:35.813931Z","shell.execute_reply.started":"2024-10-26T17:40:35.344772Z","shell.execute_reply":"2024-10-26T17:40:35.812790Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/tree-detection/data/maple/mature-sugar-maple--1024x683.jpg\n/kaggle/input/tree-detection/data/maple/Sugar-Maple_1_FGT.jpg\n/kaggle/input/tree-detection/data/maple/0000559_silver-maple_510.jpg\n/kaggle/input/tree-detection/data/maple/Acer-saccharum-Sugar-Maple-Mature-Fall-scaled.jpg\n/kaggle/input/tree-detection/data/maple/redmaple_pageimage_JanetandPhil_flickr (1).jpg\n/kaggle/input/tree-detection/data/maple/6815004908_b7bbea2119_b.jpg\n/kaggle/input/tree-detection/data/maple/acer-autumnfest-web-01.jpg\n/kaggle/input/tree-detection/data/maple/unnamed-14.jpg\n/kaggle/input/tree-detection/data/maple/gettyimages-602009593-612x612.jpg\n/kaggle/input/tree-detection/data/maple/Sugar-Maple-Green-Mountain-600x400.jpg\n/kaggle/input/tree-detection/data/maple/acer_palmatum_sango_kaku_dayton_fall_landscape_3967mnc.jpg\n/kaggle/input/tree-detection/data/maple/maple_sgr_tr_lg.jpg\n/kaggle/input/tree-detection/data/maple/4FFB0CB4-34A8-4061-B83C-B38DEBF02562.jpeg\n/kaggle/input/tree-detection/data/maple/intro-1643744576.jpg\n/kaggle/input/tree-detection/data/maple/images.jpg\n/kaggle/input/tree-detection/data/live_oak/live_oak_8.jpg\n/kaggle/input/tree-detection/data/live_oak/live_oak_11.jpg\n/kaggle/input/tree-detection/data/live_oak/live_oak_9.jpg\n/kaggle/input/tree-detection/data/live_oak/live_oak_12.jpg\n/kaggle/input/tree-detection/data/live_oak/live_oak_5.jpg\n/kaggle/input/tree-detection/data/live_oak/live_oak_14.jfif\n/kaggle/input/tree-detection/data/live_oak/live_oak_6.png\n/kaggle/input/tree-detection/data/live_oak/live_oak_1.jpg\n/kaggle/input/tree-detection/data/live_oak/live_oak_7.webp\n/kaggle/input/tree-detection/data/live_oak/live_oak_13.webp\n/kaggle/input/tree-detection/data/live_oak/live_oak_15.jpg\n/kaggle/input/tree-detection/data/live_oak/live_oak_10.jpeg\n/kaggle/input/tree-detection/data/live_oak/live_oak_3.jpg\n/kaggle/input/tree-detection/data/live_oak/live_oak_2.jpg\n/kaggle/input/tree-detection/data/live_oak/live_oak_4.jpg\n/kaggle/input/tree-detection/data/queenpalm/queen-palm.jpg\n/kaggle/input/tree-detection/data/queenpalm/Queen_Palm_2_FGT.jpg\n/kaggle/input/tree-detection/data/queenpalm/ZamjGW8fnqbbJuaPCu6r8a.jpg\n/kaggle/input/tree-detection/data/queenpalm/unnamed.jpg\n/kaggle/input/tree-detection/data/queenpalm/Starr_020617-0019_Syagrus_romanzoffiana.jpg\n/kaggle/input/tree-detection/data/queenpalm/queen-palms-500.jpg\n/kaggle/input/tree-detection/data/queenpalm/fN5FJsupYZvSNU2rZiruK4.jpg\n/kaggle/input/tree-detection/data/queenpalm/QueenPalmFlorida.jpg\n/kaggle/input/tree-detection/data/queenpalm/66b597393808fd301413940b_queenpalmtree.jpeg\n/kaggle/input/tree-detection/data/queenpalm/queen-palm-3-scaled.jpg\n/kaggle/input/tree-detection/data/queenpalm/queen-palm-07.jpg\n/kaggle/input/tree-detection/data/queenpalm/ql9e8pahp0ha1 (1).jpg\n/kaggle/input/tree-detection/data/queenpalm/il_570xN.3607220511_tj64.jpg\n/kaggle/input/tree-detection/data/queenpalm/Royal-Cuban-palms.jpg\n/kaggle/input/tree-detection/data/queenpalm/download.jpg\n/kaggle/input/tree-detection/data/eucalyptus/0003130_eucalyptus-silver-dollar.jpg\n/kaggle/input/tree-detection/data/eucalyptus/stock-photo-close-eucalyptus-tree-its-bark-peeling-blue-sky-can-seen.jpg\n/kaggle/input/tree-detection/data/eucalyptus/riverredgumlt-e1499746839509.jpg\n/kaggle/input/tree-detection/data/eucalyptus/stock-photo-majestic-eucalyptus-tree-in-the-australian-bush.jpg\n/kaggle/input/tree-detection/data/eucalyptus/81IXAew3x+L.jpg\n/kaggle/input/tree-detection/data/eucalyptus/Audreys-Garden-Debi-613x1024.jpg\n/kaggle/input/tree-detection/data/eucalyptus/Pauciflora-Alpine-Snow-Gum-Eucalyptus-Tree.jpg\n/kaggle/input/tree-detection/data/eucalyptus/images (1).jpg\n/kaggle/input/tree-detection/data/eucalyptus/Silver-Dollar-Gum-Gress-Photo-IMG_1683-2-1-scaled.jpg\n/kaggle/input/tree-detection/data/eucalyptus/eucalyptus-tree-2-jpg-webp.jpg\n/kaggle/input/tree-detection/data/eucalyptus/natural-background-part-of-eucalyptus-tree-bark-photo.jpg\n/kaggle/input/tree-detection/data/eucalyptus/IMG_20240123_132433-copy.jpeg\n/kaggle/input/tree-detection/data/eucalyptus/stock-photo-eucalyptus-tree-low-angle-view.jpg\n/kaggle/input/tree-detection/data/eucalyptus/red-gum-eucalyptus-tree-1657020876.jpg\n/kaggle/input/tree-detection/data/eucalyptus/images.jpg\n/kaggle/input/tree-detection/data/south_magnolia/south_magnolia_5.jpg\n/kaggle/input/tree-detection/data/south_magnolia/south_magnolia_9.webp\n/kaggle/input/tree-detection/data/south_magnolia/south_magnolia_11.jpeg\n/kaggle/input/tree-detection/data/south_magnolia/south_magnolia_14.jpg\n/kaggle/input/tree-detection/data/south_magnolia/south_magnolia_13.webp\n/kaggle/input/tree-detection/data/south_magnolia/south_magnolia_2.jpg\n/kaggle/input/tree-detection/data/south_magnolia/south_magnolia_8.jpg\n/kaggle/input/tree-detection/data/south_magnolia/south_magnolia_3.jpg\n/kaggle/input/tree-detection/data/south_magnolia/south_magnolia_1.jpg\n/kaggle/input/tree-detection/data/south_magnolia/south_magnolia_6.jfif\n/kaggle/input/tree-detection/data/south_magnolia/south_magnolia_15.jpg\n/kaggle/input/tree-detection/data/south_magnolia/south_magnolia_10.webp\n/kaggle/input/tree-detection/data/south_magnolia/south_magnolia_12.jpg\n/kaggle/input/tree-detection/data/south_magnolia/south_magnolia_7.jpg\n/kaggle/input/tree-detection/data/south_magnolia/south_magnolia_4.jpg\n/kaggle/input/tree-detection/data/sabel_palm/sabel_palm_5.jfif\n/kaggle/input/tree-detection/data/sabel_palm/sabel_palm_7.jfif\n/kaggle/input/tree-detection/data/sabel_palm/sabel_palm_11.jpg\n/kaggle/input/tree-detection/data/sabel_palm/sabel_palm_6.jpg\n/kaggle/input/tree-detection/data/sabel_palm/sabel_palm_4.jfif\n/kaggle/input/tree-detection/data/sabel_palm/sabel_palm_8.jpg\n/kaggle/input/tree-detection/data/sabel_palm/sabel_palm_9.webp\n/kaggle/input/tree-detection/data/sabel_palm/sabel_palm_14.jpg\n/kaggle/input/tree-detection/data/sabel_palm/sabel_palm_13.jfif\n/kaggle/input/tree-detection/data/sabel_palm/sabel_palm_2.JPG\n/kaggle/input/tree-detection/data/sabel_palm/sabel_palm_10.webp\n/kaggle/input/tree-detection/data/sabel_palm/sabel_palm_15.webp\n/kaggle/input/tree-detection/data/sabel_palm/sabel_palm_12.webp\n/kaggle/input/tree-detection/data/sabel_palm/sabel_palm_1.jpg\n/kaggle/input/tree-detection/data/sabel_palm/sabel_palm_3.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"root_path = '/kaggle/input/tree-detection/'\ndata_dir = os.path.join(root_path, \"data\")\nroot_path, data_dir","metadata":{"execution":{"iopub.status.busy":"2024-10-26T17:41:04.799251Z","iopub.execute_input":"2024-10-26T17:41:04.799795Z","iopub.status.idle":"2024-10-26T17:41:04.807843Z","shell.execute_reply.started":"2024-10-26T17:41:04.799756Z","shell.execute_reply":"2024-10-26T17:41:04.806674Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"('/kaggle/input/tree-detection/', '/kaggle/input/tree-detection/data')"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, Dataset\nimport os\nimport torch.nn as nn\nimport torch.optim as optim\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T17:40:58.043917Z","iopub.execute_input":"2024-10-26T17:40:58.044477Z","iopub.status.idle":"2024-10-26T17:41:02.838435Z","shell.execute_reply.started":"2024-10-26T17:40:58.044437Z","shell.execute_reply":"2024-10-26T17:41:02.837534Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"transformations = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),      # Flip images horizontally with 50% probability\n    transforms.RandomRotation(degrees=15),       # Rotate images randomly within a range of Â±15 degrees\n    transforms.Resize((224,224)),                # Resizing images to be consistant with each other\n    transforms.ToTensor(),                       # Convert to PyTorch tensor\n])","metadata":{"execution":{"iopub.status.busy":"2024-10-26T17:41:27.890307Z","iopub.execute_input":"2024-10-26T17:41:27.890748Z","iopub.status.idle":"2024-10-26T17:41:27.896482Z","shell.execute_reply.started":"2024-10-26T17:41:27.890708Z","shell.execute_reply":"2024-10-26T17:41:27.895459Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class CustomImageDataset(Dataset):\n    def __init__(self, data_dir, transform=None):\n        self.data_dir = data_dir\n        self.transform = transform\n        self.image_paths = []\n        self.labels = []\n        \n        # Create a mapping from folder name to label\n        self.class_to_idx = {\n            \"eucalyptus\": 0,\n            \"live_oak\": 1,\n            \"maple\": 2,\n            \"queenpalm\": 3,\n            \"sabel_palm\": 4,\n            \"south_magnolia\": 5\n            }\n        \n        # Collect all image paths and their corresponding labels\n        for folder_name, label in self.class_to_idx.items():\n            folder_path = os.path.join(data_dir, folder_name)\n            for img_file in os.listdir(folder_path):\n                img_path = os.path.join(folder_path, img_file)\n                if img_path.endswith(('.png', '.jpg', '.jpeg', '.jfif', '.webp')):  # Filter for image files\n                    self.image_paths.append(img_path)\n                    self.labels.append(label)  # Append the label associated with the folder\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        label = self.labels[idx]\n        \n        # Apply transformations if any\n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label","metadata":{"execution":{"iopub.status.busy":"2024-10-26T17:43:12.161908Z","iopub.execute_input":"2024-10-26T17:43:12.162327Z","iopub.status.idle":"2024-10-26T17:43:12.173504Z","shell.execute_reply.started":"2024-10-26T17:43:12.162287Z","shell.execute_reply":"2024-10-26T17:43:12.172406Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Initialize the dataset\ndataset = CustomImageDataset(data_dir=data_dir, transform=transformations)\n\n# Load the dataset into DataLoader\ndata_loader = DataLoader(dataset, batch_size=5, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T17:43:21.864703Z","iopub.execute_input":"2024-10-26T17:43:21.865123Z","iopub.status.idle":"2024-10-26T17:43:21.875941Z","shell.execute_reply.started":"2024-10-26T17:43:21.865082Z","shell.execute_reply":"2024-10-26T17:43:21.874914Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"num_classes = len(dataset.class_to_idx)\nnum_classes","metadata":{"execution":{"iopub.status.busy":"2024-10-26T18:13:29.597226Z","iopub.execute_input":"2024-10-26T18:13:29.597647Z","iopub.status.idle":"2024-10-26T18:13:29.604598Z","shell.execute_reply.started":"2024-10-26T18:13:29.597608Z","shell.execute_reply":"2024-10-26T18:13:29.603478Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"6"},"metadata":{}}]},{"cell_type":"code","source":"# len(dataset.class_to_idx)\n\n\nmodel = models.resnet18(pretrained=True)\n\n# Modify the final fully connected layer to match the number of classes\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T17:43:31.106199Z","iopub.execute_input":"2024-10-26T17:43:31.107156Z","iopub.status.idle":"2024-10-26T17:43:32.185496Z","shell.execute_reply.started":"2024-10-26T17:43:31.107111Z","shell.execute_reply":"2024-10-26T17:43:32.184455Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|ââââââââââ| 44.7M/44.7M [00:00<00:00, 99.1MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T17:43:35.398072Z","iopub.execute_input":"2024-10-26T17:43:35.398481Z","iopub.status.idle":"2024-10-26T17:43:35.404308Z","shell.execute_reply.started":"2024-10-26T17:43:35.398442Z","shell.execute_reply":"2024-10-26T17:43:35.403043Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\n\n# Training parameters\nnum_epochs = 20\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()  # Set the model to training mode\n    \n    running_loss = 0.0\n    for images, labels in data_loader:\n        images, labels = images.to(device), labels.to(device)\n        \n        # Zero the parameter gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * images.size(0)\n    \n    epoch_loss = running_loss / len(dataset)\n    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T17:43:40.170441Z","iopub.execute_input":"2024-10-26T17:43:40.170835Z","iopub.status.idle":"2024-10-26T17:44:47.359793Z","shell.execute_reply.started":"2024-10-26T17:43:40.170795Z","shell.execute_reply":"2024-10-26T17:44:47.357726Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Epoch 1/20, Loss: 1.4594\nEpoch 2/20, Loss: 1.4959\nEpoch 3/20, Loss: 1.3008\nEpoch 4/20, Loss: 0.9315\nEpoch 5/20, Loss: 0.6534\nEpoch 6/20, Loss: 0.9093\nEpoch 7/20, Loss: 0.9512\nEpoch 8/20, Loss: 0.9172\nEpoch 9/20, Loss: 0.7626\nEpoch 10/20, Loss: 0.7653\nEpoch 11/20, Loss: 0.6880\nEpoch 12/20, Loss: 0.7912\nEpoch 13/20, Loss: 0.6457\nEpoch 14/20, Loss: 0.4743\nEpoch 15/20, Loss: 0.4775\nEpoch 16/20, Loss: 0.8817\nEpoch 17/20, Loss: 0.4378\nEpoch 18/20, Loss: 0.3124\nEpoch 19/20, Loss: 0.2327\nEpoch 20/20, Loss: 0.3046\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), \"tree_detection_model2.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-10-26T17:45:01.897274Z","iopub.execute_input":"2024-10-26T17:45:01.897996Z","iopub.status.idle":"2024-10-26T17:45:01.974106Z","shell.execute_reply.started":"2024-10-26T17:45:01.897943Z","shell.execute_reply":"2024-10-26T17:45:01.973178Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## **Training Using EfficientNet**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, Dataset\nimport os\nimport torch.nn as nn\nimport torch.optim as optim\nfrom PIL import Image\n\n# Define root paths\nroot_path = '/kaggle/input/tree-detection/'\ndata_dir = os.path.join(root_path, \"data\")\n\n# Data transformations with augmentation and normalization\ntransformations = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(degrees=15),\n    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n    transforms.RandomVerticalFlip(p=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize for pretrained model compatibility\n])\n\n# Custom dataset class\nclass CustomImageDataset(Dataset):\n    def __init__(self, data_dir, transform=None):\n        self.data_dir = data_dir\n        self.transform = transform\n        self.image_paths = []\n        self.labels = []\n        \n        # Create a mapping from folder name to label\n        self.class_to_idx = {\n            \"eucalyptus\": 0,\n            \"live_oak\": 1,\n            \"maple\": 2,\n            \"queenpalm\": 3,\n            \"sabel_palm\": 4,\n            \"south_magnolia\": 5\n        }\n        \n        # Collect all image paths and their corresponding labels\n        for folder_name, label in self.class_to_idx.items():\n            folder_path = os.path.join(data_dir, folder_name)\n            for img_file in os.listdir(folder_path):\n                img_path = os.path.join(folder_path, img_file)\n                if img_path.endswith(('.png', '.jpg', '.jpeg', '.jfif', '.webp')):\n                    self.image_paths.append(img_path)\n                    self.labels.append(label)\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        label = self.labels[idx]\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label\n\n# Initialize the dataset and DataLoader\ndataset = CustomImageDataset(data_dir=data_dir, transform=transformations)\ndata_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n\nnum_classes = len(dataset.class_to_idx)\n\n# Load EfficientNet model for better accuracy\nmodel = models.efficientnet_b0(pretrained=True)\nmodel.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n\n# Move the model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Define loss function and optimizer with learning rate scheduler\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\n# Training parameters\nnum_epochs = 20\n\n# Fine-tune all layers\nfor param in model.parameters():\n    param.requires_grad = True\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for images, labels in data_loader:\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()  # Zero the parameter gradients\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * images.size(0)\n    \n    epoch_loss = running_loss / len(dataset)\n    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n    \n    # Step the learning rate scheduler\n    scheduler.step()\n\n# Save the model\ntorch.save(model.state_dict(), \"tree_detection_model_optimized.pth\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T18:25:22.545390Z","iopub.execute_input":"2024-10-26T18:25:22.545743Z","iopub.status.idle":"2024-10-26T18:25:58.742646Z","shell.execute_reply.started":"2024-10-26T18:25:22.545711Z","shell.execute_reply":"2024-10-26T18:25:58.741407Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n100%|ââââââââââ| 20.5M/20.5M [00:00<00:00, 125MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20, Loss: 1.6370\nEpoch 2/20, Loss: 0.6404\nEpoch 3/20, Loss: 0.1818\nEpoch 4/20, Loss: 0.0591\nEpoch 5/20, Loss: 0.0151\nEpoch 6/20, Loss: 0.0089\nEpoch 7/20, Loss: 0.0026\nEpoch 8/20, Loss: 0.0728\nEpoch 9/20, Loss: 0.0040\nEpoch 10/20, Loss: 0.0070\nEpoch 11/20, Loss: 0.0076\nEpoch 12/20, Loss: 0.0567\nEpoch 13/20, Loss: 0.0020\nEpoch 14/20, Loss: 0.0029\nEpoch 15/20, Loss: 0.0178\nEpoch 16/20, Loss: 0.0116\nEpoch 17/20, Loss: 0.0016\nEpoch 18/20, Loss: 0.0013\nEpoch 19/20, Loss: 0.0011\nEpoch 20/20, Loss: 0.0345\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## **Training and Validation Using EfficientNet**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, Dataset, random_split\nimport os\nimport torch.nn as nn\nimport torch.optim as optim\nfrom PIL import Image\n\n# Define root paths\nroot_path = '/kaggle/input/tree-detection/'\ndata_dir = os.path.join(root_path, \"data\")\n\n# Data transformations with augmentation and normalization\ntransformations = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(degrees=15),\n    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n    transforms.RandomVerticalFlip(p=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize for pretrained model compatibility\n])\n\n# Custom dataset class\nclass CustomImageDataset(Dataset):\n    def __init__(self, data_dir, transform=None):\n        self.data_dir = data_dir\n        self.transform = transform\n        self.image_paths = []\n        self.labels = []\n        \n        # Create a mapping from folder name to label\n        self.class_to_idx = {\n            \"eucalyptus\": 0,\n            \"live_oak\": 1,\n            \"maple\": 2,\n            \"queenpalm\": 3,\n            \"sabel_palm\": 4,\n            \"south_magnolia\": 5\n        }\n        \n        # Collect all image paths and their corresponding labels\n        for folder_name, label in self.class_to_idx.items():\n            folder_path = os.path.join(data_dir, folder_name)\n            for img_file in os.listdir(folder_path):\n                img_path = os.path.join(folder_path, img_file)\n                if img_path.endswith(('.png', '.jpg', '.jpeg', '.jfif', '.webp')):\n                    self.image_paths.append(img_path)\n                    self.labels.append(label)\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        label = self.labels[idx]\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label\n\n# Initialize the dataset\ndataset = CustomImageDataset(data_dir=data_dir, transform=transformations)\n\n# Split the dataset into training and validation sets (80% training, 20% validation)\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n# Create DataLoaders for training and validation\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n\nnum_classes = len(dataset.class_to_idx)\n\n# Load EfficientNet model for better accuracy\nmodel = models.efficientnet_b0(pretrained=True)\nmodel.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n\n# Move the model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Define loss function and optimizer with learning rate scheduler\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\n# Training parameters\nnum_epochs = 20\n\n# Fine-tune all layers\nfor param in model.parameters():\n    param.requires_grad = True\n\n# Training and validation loop\nfor epoch in range(num_epochs):\n    # Training phase\n    model.train()\n    running_loss = 0.0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()  # Zero the parameter gradients\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * images.size(0)\n    \n    epoch_loss = running_loss / len(train_dataset)\n    \n    # Validation phase\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    val_accuracy = 100 * correct / total\n    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n    \n    # Step the learning rate scheduler\n    scheduler.step()\n\n# Save the model\ntorch.save(model.state_dict(), \"tree_detection_model_optimized_eval.pth\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T19:02:28.205174Z","iopub.execute_input":"2024-10-26T19:02:28.206157Z","iopub.status.idle":"2024-10-26T19:03:17.993299Z","shell.execute_reply.started":"2024-10-26T19:02:28.206111Z","shell.execute_reply":"2024-10-26T19:03:17.992296Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Epoch 1/20, Loss: 1.7422, Validation Accuracy: 66.67%\nEpoch 2/20, Loss: 0.8606, Validation Accuracy: 83.33%\nEpoch 3/20, Loss: 0.3314, Validation Accuracy: 83.33%\nEpoch 4/20, Loss: 0.1019, Validation Accuracy: 94.44%\nEpoch 5/20, Loss: 0.0530, Validation Accuracy: 100.00%\nEpoch 6/20, Loss: 0.0293, Validation Accuracy: 72.22%\nEpoch 7/20, Loss: 0.0807, Validation Accuracy: 83.33%\nEpoch 8/20, Loss: 0.0431, Validation Accuracy: 77.78%\nEpoch 9/20, Loss: 0.0517, Validation Accuracy: 88.89%\nEpoch 10/20, Loss: 0.0666, Validation Accuracy: 94.44%\nEpoch 11/20, Loss: 0.1550, Validation Accuracy: 100.00%\nEpoch 12/20, Loss: 0.0625, Validation Accuracy: 88.89%\nEpoch 13/20, Loss: 0.0678, Validation Accuracy: 94.44%\nEpoch 14/20, Loss: 0.0891, Validation Accuracy: 88.89%\nEpoch 15/20, Loss: 0.0626, Validation Accuracy: 88.89%\nEpoch 16/20, Loss: 0.0508, Validation Accuracy: 94.44%\nEpoch 17/20, Loss: 0.0717, Validation Accuracy: 88.89%\nEpoch 18/20, Loss: 0.0800, Validation Accuracy: 88.89%\nEpoch 19/20, Loss: 0.0157, Validation Accuracy: 94.44%\nEpoch 20/20, Loss: 0.0154, Validation Accuracy: 88.89%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}